{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing and loading packages"
      ],
      "metadata": {
        "id": "ozUnszsNmSiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate gradio torch"
      ],
      "metadata": {
        "id": "8YB5WDZbrPXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model & tokenizer"
      ],
      "metadata": {
        "id": "6DFA5j2YuVxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"gpt2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model and tokenizer loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wkDqh4IatATe",
        "outputId": "4863892b-e037-4378-be28-c85d303bf457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Showing List of input tokens and top 10 suggested tokens"
      ],
      "metadata": {
        "id": "K5DEyRmF1JPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "def show_tokenization_and_next_probs(prompt, top_k=10):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)  #Tokenizes the prompt text (converts it to numeric tokens)\n",
        "    input_ids = inputs[\"input_ids\"]                             #Contains the numeric identifier of each token (list of numbers).\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])      #Converts any number into a readable text token.\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"IDs:\", input_ids[0].tolist())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids)  #It runs the model and the output includes logits.\n",
        "        logits = outputs.logits               # shape: (1, seq_len, vocab_size)\n",
        "    next_logits = logits[0, -1, :]            #Getting logits for the last token (From the first batch item, take the last input token, and select the total output of its probabilities for all vocabulary)\n",
        "    probs = F.softmax(next_logits, dim=-1)    #Calculating words probabilities with Softmax\n",
        "\n",
        "    top = torch.topk(probs, k=top_k)          #from all the tokens, it returns K(10) with the highest probability.\n",
        "    print(f\"\\nTop {top_k} candidate next tokens (id -> token -> prob):\")\n",
        "    for idx, p in zip(top.indices.tolist(), top.values.tolist()):\n",
        "        print(f\"{idx} -> {repr(tokenizer.decode([idx]))} -> {p:.4f}\") #Converts any ID to text.\n",
        "\n",
        "show_tokenization_and_next_probs(\"The future of AI is\", top_k=10) #Specifies how many of the next most likely tokens to show."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxMxNn7wtbfQ",
        "outputId": "32e37525-d693-42c5-8c6d-30a6d901ca53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'Ġfuture', 'Ġof', 'ĠAI', 'Ġis']\n",
            "IDs: [464, 2003, 286, 9552, 318]\n",
            "\n",
            "Top 10 candidate next tokens (id -> token -> prob):\n",
            "8627 -> ' uncertain' -> 0.0613\n",
            "287 -> ' in' -> 0.0586\n",
            "407 -> ' not' -> 0.0451\n",
            "257 -> ' a' -> 0.0403\n",
            "991 -> ' still' -> 0.0364\n",
            "1016 -> ' going' -> 0.0247\n",
            "845 -> ' very' -> 0.0190\n",
            "783 -> ' now' -> 0.0186\n",
            "10061 -> ' unclear' -> 0.0182\n",
            "379 -> ' at' -> 0.0171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual Greedy implementation (token-by-token)"
      ],
      "metadata": {
        "id": "QvXZ1Btb543d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greedily means that at each step it only selects the highest probability (largest logit). So Each time the model generates a new word, it adds it to the sentence."
      ],
      "metadata": {
        "id": "0zuVqeO520ZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greedy Always chooses the most likely option. | it is Quick, simple | Repetitive and monotonous, low creativity"
      ],
      "metadata": {
        "id": "W7h4ROAW6e0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def greedy_token_by_token(prompt, max_new_tokens=30):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) #Tokenizes the prompt text (converts it to numeric tokens)\n",
        "    generated = input_ids       #Initially it only contains the initial prompt tokens. Later we will add new tokens to it at each step.\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=generated)\n",
        "            next_token_logits = outputs.logits[0, -1, :]\n",
        "            next_token_id = torch.argmax(next_token_logits).unsqueeze(0).unsqueeze(0)  # shape (1,1)\n",
        "        generated = torch.cat([generated, next_token_id.to(device)], dim=1)   #Add a new token to the sequence\n",
        "\n",
        "        if next_token_id.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "    return tokenizer.decode(generated[0], skip_special_tokens=True)   #Convert the result to final text |||| skip_special_tokens=True → Removes special tokens like <pad>, <eos>, <bos>.\n",
        "\n",
        "\n",
        "print(greedy_token_by_token(\"The future of AI is \", max_new_tokens=30))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSjHSba2tgD5",
        "outputId": "e76da94d-a105-4383-e3f2-338d880fa5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of AI is  a matter of debate. The question is whether we can make it work. The answer is yes.\n",
            "The future of AI is  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation with three different strategies using model.generate:"
      ],
      "metadata": {
        "id": "F7qlAK74_peh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greedy Search → Simple and definitive |\n",
        "Beam Search → More precise with multiple branches |\n",
        "Sampling (Top-k / Top-p) → Creative and diverse"
      ],
      "metadata": {
        "id": "G2xqVWvcOe_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_strategy(prompt,\n",
        "                           strategy=\"greedy\",           # \"greedy\", \"beam\", \"sampling\"\n",
        "                           max_new_tokens=50,\n",
        "                           num_return_sequences=1,      # How many different text completions to return\n",
        "                         #When using beam, usually num_return_sequences ≤ num_beams\n",
        "                           num_beams=5,                 #num_beams = 1 → same as greedy (fast and simple).  | num_beams = 2..5 → often the best quality/speed balance for short to medium texts.\n",
        "                           repetition_penalty=1.0,      #Values ​​greater than 1.0 penalize tokens that have already appeared in the text; values ​​less than 1.0 encourage repetition.\n",
        "                           temperature=1.0,             # Controls randomness in Sampling (only used if do_sample=True). Lower = more focused, higher = more creative\n",
        "                           top_k=50,                    #Only select the token with the highest probability from the top-k.\n",
        "                           top_p=1.0,                   #Or among tokens whose sum of probabilities is ≤ p\n",
        "                           seed=None):                  #To ensure that the random output in Sampling is the same every time.\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    # Creating a settings dictionary for generate()\n",
        "    gen_kwargs = {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"pad_token_id\": tokenizer.eos_token_id,\n",
        "        \"num_return_sequences\": num_return_sequences\n",
        "    }\n",
        "\n",
        "    # Quick and simple, but sometimes repetitive and monotonous.\n",
        "    if strategy == \"greedy\":\n",
        "        gen_kwargs.update({\"do_sample\": False, \"num_beams\": 1})     #No accidents — just the highest probability at each step.\n",
        "\n",
        "\n",
        "    # More accurate than Greedy, but slower. It's like the model thinks a few options ahead before making a decision.\n",
        "    elif strategy == \"beam\":\n",
        "        gen_kwargs.update({\"do_sample\": False, \"num_beams\": num_beams,\n",
        "                           \"repetition_penalty\": repetition_penalty, \"early_stopping\": True}) #It stops when all branches have produced EOS.\n",
        "\n",
        "\n",
        "    #Creative and diverse, for more natural and non-repetitive texts. But it can sometimes become illogical if the parameters are not adjusted too much.\n",
        "    elif strategy == \"sampling\":\n",
        "        gen_kwargs.update({\"do_sample\": True, \"temperature\": temperature,\n",
        "                           \"top_k\": top_k if top_k>0 else None, \"top_p\": top_p})\n",
        "    else:\n",
        "        raise ValueError(\"strategy must be 'greedy'|'beam'|'sampling'\")\n",
        "\n",
        "    # Remove any keys from gen_kwargs whose values are None\n",
        "    gen_kwargs = {k:v for k,v in gen_kwargs.items() if v is not None}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**gen_kwargs)\n",
        "\n",
        "    return [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
        "\n",
        "\n",
        "\n",
        "prompt = \"The future of AI is\"\n",
        "print(\"\\n-- Greedy --\") # Definite, logical, but repetitive\n",
        "print(generate_with_strategy(prompt, strategy=\"greedy\", max_new_tokens=30)[0])\n",
        "\n",
        "print(\"\\n-- Beam (num_beams=5) --\") # More natural and balanced\n",
        "print(generate_with_strategy(prompt, strategy=\"beam\", num_beams=5, max_new_tokens=30)[0])\n",
        "\n",
        "print(\"\\n-- Sampling (temp=1.2, top_k=50) --\")  #Creative, sometimes unexpected\n",
        "print(generate_with_strategy(prompt, strategy=\"sampling\", temperature=1.2, top_k=50, top_p=0.95, max_new_tokens=50)[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS98zyd7-MRA",
        "outputId": "9f1b3342-7191-47c1-e76b-3fdd20d3d602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Greedy --\n",
            "The future of AI is uncertain. The future of AI is uncertain.\n",
            "\n",
            "The future of AI is uncertain. The future of AI is uncertain.\n",
            "\n",
            "The future of\n",
            "\n",
            "-- Beam (num_beams=5) --\n",
            "The future of AI is in the hands of the next generation of scientists and engineers.\n",
            "\n",
            "The future of AI is in the hands of the next generation of scientists and engineers\n",
            "\n",
            "-- Sampling (temp=1.2, top_k=50) --\n",
            "The future of AI is something that is open now. We know that in 2014 we had AI in every aspect of our lives. Even we're all in the same room. Now we can actually talk to AI from anywhere in the world. And that could become your world,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization with Gradio"
      ],
      "metadata": {
        "id": "CeDzJsvAMBDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generate_gradio(prompt, strategy, max_new_tokens, num_return_sequences,\n",
        "                    num_beams, repetition_penalty, temperature, top_k, top_p, seed):\n",
        "    strat = {\"Greedy\":\"greedy\",\"Beam\":\"beam\",\"Sampling\":\"sampling\"}[strategy]\n",
        "\n",
        "    # Calling the text generation function:\n",
        "    outs = generate_with_strategy(prompt,\n",
        "                                  strategy=strat,\n",
        "                                  max_new_tokens=int(max_new_tokens),\n",
        "                                  num_return_sequences=int(num_return_sequences),\n",
        "                                  num_beams=int(num_beams),\n",
        "                                  repetition_penalty=float(repetition_penalty),\n",
        "                                  temperature=float(temperature),\n",
        "                                  top_k=int(top_k),\n",
        "                                  top_p=float(top_p),\n",
        "                                  seed=int(seed) if seed!=\"\" else None)\n",
        "    return \"\\n\\n---\\n\\n\".join(outs)\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=generate_gradio,\n",
        "    inputs=[\n",
        "        gr.Textbox(value=\"The future of AI is\", label=\"Prompt\"),\n",
        "        gr.Radio([\"Greedy\",\"Beam\",\"Sampling\"], value=\"Greedy\", label=\"Strategy\"),\n",
        "        gr.Slider(1, 200, value=50, label=\"max_new_tokens\"),\n",
        "        gr.Slider(1, 5, value=1, step=1, label=\"num_return_sequences\"),\n",
        "        gr.Slider(1, 10, value=5, step=1, label=\"num_beams\"),\n",
        "        gr.Slider(0.7, 2.0, value=1.0, step=0.1, label=\"repetition_penalty\"),\n",
        "        gr.Slider(0.1, 2.0, value=1.0, step=0.1, label=\"temperature\"),\n",
        "        gr.Slider(0, 200, value=50, step=1, label=\"top_k\"),\n",
        "        gr.Slider(0.0, 1.0, value=1.0, step=0.01, label=\"top_p\"),\n",
        "        gr.Textbox(value=\"\", label=\"seed (optional)\")\n",
        "    ],\n",
        "    outputs=\"text\",   # The output is just text.\n",
        "    title=\"Text Generation Explorer (gpt2)\"\n",
        ")\n",
        "\n",
        "\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "dypXIXRgLk4U",
        "outputId": "d63a9be3-eaf9-445c-8104-134401729de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://30723667ce82286160.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://30723667ce82286160.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}